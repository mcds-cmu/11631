---
layout: f23
title: Reading List
nav_order: 4
---

<table class="tg">
<thead>
  <tr>
    <th class="tg-sx1p"><span style="font-weight:600">Required Reading</span></th>
    <th class="tg-sx1p"><span style="font-weight:600">Example Prior Work:</span></th>
    <th class="tg-sx1p"><span style="font-weight:600">Example Follow-up Work:</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax"><a href="https://arxiv.org/pdf/1803.03635.pdf" target="_blank" rel="noopener noreferrer"><span style="text-decoration:none">Frankle, J., &amp; Carbin, M. (2018). “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” arXiv preprint arXiv:1803.03635.</span></a></td>
    <td class="tg-0lax"><a href="https://papers.nips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf" target="_blank" rel="noopener noreferrer">Optimal Brain Damage. Yann LeCun, John Denker, Sara Solla. NIPS 1989.</a><br><br><a href="https://papers.nips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html" target="_blank" rel="noopener noreferrer">Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally. NIPS 2015.</a><br><br><a href="https://openreview.net/forum?id=rJqFGTslg" target="_blank" rel="noopener noreferrer"><span style="text-decoration:none">Pruning Filters for Efficient ConvNets. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf. ICLR 2017.</span></a></td>
    <td class="tg-0lax"><a href="https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html" target="_blank" rel="noopener noreferrer">The Lottery Ticket Hypothesis for Pre-trained BERT Networks. Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin. NeurIPS 2020.</a><br><br><a href="https://proceedings.mlr.press/v119/frankle20a.html" target="_blank" rel="noopener noreferrer">Linear Mode Connectivity and the Lottery Ticket Hypothesis. Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin. ICML 2020.</a><br><br><a href="https://openreview.net/forum?id=BJxsrgStvr" target="_blank" rel="noopener noreferrer">Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks. Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G. Baraniuk, Zhangyang Wang, Yingyan Lin. ICLR 2020.</a><br><br><a href="https://proceedings.mlr.press/v119/malach20a.html" target="_blank" rel="noopener noreferrer">Proving the Lottery Ticket Hypothesis: Pruning is All You Need. Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, Ohad Shamir. ICML 2020.</a><br><br><a href="https://openreview.net/forum?id=Hkl1iRNFwS" target="_blank" rel="noopener noreferrer"><span style="text-decoration:none">The Early Phase of Neural Network Training. Jonathan Frankle, David J. Schwab, Ari S. Morcos. ICLR 2020.</span></a></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://arxiv.org/pdf/1707.09457.pdf" target="_blank" rel="noopener noreferrer"><span style="text-decoration:none">Zhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. (2017). "Men also like shopping: Reducing gender bias amplification using corpus-level constraints." arXiv preprint arXiv:1707.09457.</span></a></td>
    <td class="tg-cly1"></td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://arxiv.org/pdf/1806.00692.pdf" target="_blank" rel="noopener noreferrer"><span style="text-decoration:none">“Stress Test Evaluation for Natural Language Inference.” Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig. 27th International Conference on Computational Linguistics (COLING-2018)</span></a></td>
    <td class="tg-cly1"></td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://arxiv.org/abs/1812.05239">Ken Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudík, Hanna Wallach, Improving fairness in machine learning systems: What do industry practitioners need?, in Proceedings of 2019 ACM CHI Conference on Human Factors in Computing Systems.</a></td>
    <td class="tg-cly1"></td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://jmlr.org/papers/volume21/20-312/20-312.pdf">Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, Joelle Pineau. Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning, JMLR 2020.</a></td>
    <td class="tg-cly1"></td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://openreview.net/pdf?id=GKTvAcb12b">Emily M. Bender and Alexander Koller, Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198.</a></td>
    <td class="tg-cly1"></td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax"><a href="https://proceedings.mlsys.org/paper/2022/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf">Kuchnik, M., Klimovic, A., Simsa, J., Smith, V., & Amvrosiadis, G. (2022). Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines. Proceedings of Machine Learning and Systems, 4, 33-51.</a></td>
    <td class="tg-cly1"></td>
    <td class="tg-0lax"></td>
  </tr>
</tbody>
</table>